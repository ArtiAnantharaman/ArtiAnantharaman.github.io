---
layout: post
title: 'Planar Homography'
---

In this programming assignment, I implemented an interest point (keypoint) detector, a feature descriptor and an image stitching tool based on feature matching and homography. Interest point (keypoint) detectors find particularly salient points in an image. We
can then extract a feature descriptor that helps describe the region around each of the interest points. SIFT, SURF, and BRIEF are all examples of commonly used feature descriptors. Once we have extracted the interest points, we can use feature descriptors to match them (find correspondences) between images to do interesting things like panorama stitching or scene reconstruction.

I implemented the <font color = "black"><b>BRIEF feature descriptor</b></font> in this homework. It has a compact representation, is quick to compute, has a discriminative yet easily computed distance metric, and is relatively simple to implement. This allows for real-time computation, as
you have seen in class. Most importantly, as you will see, it is also just as powerful as more complex descriptors like SIFT for many cases. After matching the features that we extract, I explored the homography between images based on the locations of the matched features. Why is this useful? In many robotics applications, robots must often deal with tabletops, ground, and walls among other flat planar surfaces. When two cameras observe a plane, there exists a relationship between the captured images. This
relationship is defined by a 3 × 3 transformation matrix, called a <font color = "black"><b>planar homography</b></font>. A planar homography allows us to compute how a planar scene would look from a second camera location, given only the first camera image. We can compute how images of the planes will look from any camera at any location without knowing any internal camera parameters and without actually taking the pictures, all using the planar homography matrix.

Keypoints are found by using the <font color = "black"><b>Difference of Gaussian (DoG) detector</b></font>. This detector finds points that are extrema in both scale and space of a DoG pyramid. To create a DoG pyramid, we will first need to create a Gaussian pyramid. Gaussian pyramids are constructed by progressively applying a low pass Gaussian filter to the input image. The Difference of Gaussian function responds strongly on corners and edges in addition to blob-like objects. However, edges are not desirable for feature extraction as they are not as distinctive and do not provide a substantially stable localization for keypoints. Edges were removed using the technique of principal curvature ratio in a local neighborhood of a point. To detect corner-like, scale-invariant interest points, the DoG detector chooses points that are local extrema in both scale and space. Here, I considered a point’s eight neighbors in space and its two neighbors in scale (one in the scale above and one in the scale below).

To find the most informative feature points in the image, we can compute descriptors that can be used to match to other
views of the same point in different images. The BRIEF descriptor encodes information from a 9 × 9 patch p centered around the interest point at the characteristic scale of the interest point. A descriptor’s strength is in its ability to match to other descriptors generated by the same world point despite a change of view, lighting, etc. The distance metric used to
compute the similarity between two descriptors is critical. For BRIEF, this distance metric is the Hamming distance. The Hamming distance is simply the number of bits in two descriptors that differ. (Note that the position of the bits matters).

The least squares method that is commonly employed for computing homographies is not robust to outliers. If all the correspondences are good matches, this is not a problem. But even a single false correspondence can completely throw off the homography estimation. When correspondences are determined automatically (using BRIEF feature matching for instance), some mismatches in a set of point correspondences are almost certain. <font color = "black"><b>RANSAC(Random Sample Consensus)</b></font> can be used to fit models robustly in the presence of outliers.

The figure below shows the "stitched" panorama of the Dusquesne Incline obtained from the homography relationship between two images:
<img src="/assets/img/projects/proj-9/panorama.jpg" alt="panorama">

We can also use homographies to create a panorama image from multiple views of the same scene. This is possible for example when there is no camera translation between the views (e.g., only rotation about the camera center). First, I generated panoramas using matched point correspondences between images using the BRIEF matching. The homography between two images of the same planar scene of the Dusquesne Incline in Pittsburgh was estimated and "stitched" together into a single <font color = "black"><b>panorama</b></font>. Thus, the script accepts two images as input, computes keypoints and descriptors for both the images, finds putative feature correspondences by matching keypoint
descriptors, estimates a homography using RANSAC and then warps one of the images with the homography so that they are aligned and then overlays them.